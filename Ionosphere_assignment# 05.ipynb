{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-ahnoT3rQZ67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cvZ6I8-0B-F"
   },
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_VymCiVaQdBm"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/ramsha275/ML_Datasets/main/ionosphere_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "EUJatlivQj1r",
    "outputId": "e60eba82-963f-476e-bdc5-17829e56594e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>feature31</th>\n",
       "      <th>feature32</th>\n",
       "      <th>feature33</th>\n",
       "      <th>feature34</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83508</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.73739</td>\n",
       "      <td>-0.14706</td>\n",
       "      <td>0.84349</td>\n",
       "      <td>-0.05567</td>\n",
       "      <td>0.90441</td>\n",
       "      <td>-0.04622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04202</td>\n",
       "      <td>0.83479</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12815</td>\n",
       "      <td>0.86660</td>\n",
       "      <td>-0.10714</td>\n",
       "      <td>0.90546</td>\n",
       "      <td>-0.04307</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95113</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.95183</td>\n",
       "      <td>-0.02723</td>\n",
       "      <td>0.93438</td>\n",
       "      <td>-0.01920</td>\n",
       "      <td>0.94590</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01361</td>\n",
       "      <td>0.93522</td>\n",
       "      <td>0.04925</td>\n",
       "      <td>0.93159</td>\n",
       "      <td>0.08168</td>\n",
       "      <td>0.94066</td>\n",
       "      <td>-0.00035</td>\n",
       "      <td>0.91483</td>\n",
       "      <td>0.04712</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>-0.00034</td>\n",
       "      <td>0.93207</td>\n",
       "      <td>-0.03227</td>\n",
       "      <td>0.95177</td>\n",
       "      <td>-0.03431</td>\n",
       "      <td>0.95584</td>\n",
       "      <td>0.02446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>0.92489</td>\n",
       "      <td>0.02542</td>\n",
       "      <td>0.92120</td>\n",
       "      <td>0.02242</td>\n",
       "      <td>0.92459</td>\n",
       "      <td>0.00442</td>\n",
       "      <td>0.92697</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90608</td>\n",
       "      <td>-0.01657</td>\n",
       "      <td>0.98122</td>\n",
       "      <td>-0.01989</td>\n",
       "      <td>0.95691</td>\n",
       "      <td>-0.03646</td>\n",
       "      <td>0.85746</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02099</td>\n",
       "      <td>0.89147</td>\n",
       "      <td>-0.07760</td>\n",
       "      <td>0.82983</td>\n",
       "      <td>-0.17238</td>\n",
       "      <td>0.96022</td>\n",
       "      <td>-0.03757</td>\n",
       "      <td>0.87403</td>\n",
       "      <td>-0.16243</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84710</td>\n",
       "      <td>0.13533</td>\n",
       "      <td>0.73638</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>0.87873</td>\n",
       "      <td>0.08260</td>\n",
       "      <td>0.88928</td>\n",
       "      <td>-0.09139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15114</td>\n",
       "      <td>0.81147</td>\n",
       "      <td>-0.04822</td>\n",
       "      <td>0.78207</td>\n",
       "      <td>-0.00703</td>\n",
       "      <td>0.75747</td>\n",
       "      <td>-0.06678</td>\n",
       "      <td>0.85764</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0           1         0   0.99539  -0.05889   0.85243   0.02306   0.83398   \n",
       "1           1         0   1.00000  -0.18829   0.93035  -0.36156  -0.10868   \n",
       "2           1         0   1.00000  -0.03365   1.00000   0.00485   1.00000   \n",
       "3           1         0   1.00000  -0.45161   1.00000   1.00000   0.71216   \n",
       "4           1         0   1.00000  -0.02401   0.94140   0.06531   0.92106   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "346         1         0   0.83508   0.08298   0.73739  -0.14706   0.84349   \n",
       "347         1         0   0.95113   0.00419   0.95183  -0.02723   0.93438   \n",
       "348         1         0   0.94701  -0.00034   0.93207  -0.03227   0.95177   \n",
       "349         1         0   0.90608  -0.01657   0.98122  -0.01989   0.95691   \n",
       "350         1         0   0.84710   0.13533   0.73638  -0.06151   0.87873   \n",
       "\n",
       "     feature8  feature9  feature10  ...  feature26  feature27  feature28  \\\n",
       "0    -0.37708   1.00000    0.03760  ...   -0.51171    0.41078   -0.46168   \n",
       "1    -0.93597   1.00000   -0.04549  ...   -0.26569   -0.20468   -0.18401   \n",
       "2    -0.12062   0.88965    0.01198  ...   -0.40220    0.58984   -0.22145   \n",
       "3    -1.00000   0.00000    0.00000  ...    0.90695    0.51613    1.00000   \n",
       "4    -0.23255   0.77152   -0.16399  ...   -0.65158    0.13290   -0.53206   \n",
       "..        ...       ...        ...  ...        ...        ...        ...   \n",
       "346  -0.05567   0.90441   -0.04622  ...   -0.04202    0.83479    0.00123   \n",
       "347  -0.01920   0.94590    0.01606  ...    0.01361    0.93522    0.04925   \n",
       "348  -0.03431   0.95584    0.02446  ...    0.03193    0.92489    0.02542   \n",
       "349  -0.03646   0.85746    0.00110  ...   -0.02099    0.89147   -0.07760   \n",
       "350   0.08260   0.88928   -0.09139  ...   -0.15114    0.81147   -0.04822   \n",
       "\n",
       "     feature29  feature30  feature31  feature32  feature33  feature34  label  \n",
       "0      0.21266   -0.34090    0.42267   -0.54487    0.18641   -0.45300      g  \n",
       "1     -0.19040   -0.11593   -0.16626   -0.06288   -0.13738   -0.02447      b  \n",
       "2      0.43100   -0.17365    0.60436   -0.24180    0.56045   -0.38238      g  \n",
       "3      1.00000   -0.20099    0.25682    1.00000   -0.32382    1.00000      b  \n",
       "4      0.02431   -0.62197   -0.05707   -0.59573   -0.04608   -0.65697      g  \n",
       "..         ...        ...        ...        ...        ...        ...    ...  \n",
       "346    1.00000    0.12815    0.86660   -0.10714    0.90546   -0.04307      g  \n",
       "347    0.93159    0.08168    0.94066   -0.00035    0.91483    0.04712      g  \n",
       "348    0.92120    0.02242    0.92459    0.00442    0.92697   -0.00577      g  \n",
       "349    0.82983   -0.17238    0.96022   -0.03757    0.87403   -0.16243      g  \n",
       "350    0.78207   -0.00703    0.75747   -0.06678    0.85764   -0.06151      g  \n",
       "\n",
       "[351 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DJpmlnY0GnV"
   },
   "source": [
    "# **Check Missing Values ( If Exist ; Fill each record with mean of its feature ) or any usless column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPdC24-AQk_z",
    "outputId": "01677320-2d74-4eae-979e-0ca336e75311"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     False\n",
       "feature2     False\n",
       "feature3     False\n",
       "feature4     False\n",
       "feature5     False\n",
       "feature6     False\n",
       "feature7     False\n",
       "feature8     False\n",
       "feature9     False\n",
       "feature10    False\n",
       "feature11    False\n",
       "feature12    False\n",
       "feature13    False\n",
       "feature14    False\n",
       "feature15    False\n",
       "feature16    False\n",
       "feature17    False\n",
       "feature18    False\n",
       "feature19    False\n",
       "feature20    False\n",
       "feature21    False\n",
       "feature22    False\n",
       "feature23    False\n",
       "feature24    False\n",
       "feature25    False\n",
       "feature26    False\n",
       "feature27    False\n",
       "feature28    False\n",
       "feature29    False\n",
       "feature30    False\n",
       "feature31    False\n",
       "feature32    False\n",
       "feature33    False\n",
       "feature34    False\n",
       "label        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "S2soWTpYQua-",
    "outputId": "a9e2a01f-bb7a-4498-93e9-c9fb48c79d19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>feature31</th>\n",
       "      <th>feature32</th>\n",
       "      <th>feature33</th>\n",
       "      <th>feature34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.0</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.891738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.641342</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>0.601068</td>\n",
       "      <td>0.115889</td>\n",
       "      <td>0.550095</td>\n",
       "      <td>0.119360</td>\n",
       "      <td>0.511848</td>\n",
       "      <td>0.181345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396135</td>\n",
       "      <td>-0.071187</td>\n",
       "      <td>0.541641</td>\n",
       "      <td>-0.069538</td>\n",
       "      <td>0.378445</td>\n",
       "      <td>-0.027907</td>\n",
       "      <td>0.352514</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>0.349364</td>\n",
       "      <td>0.014480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.311155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497708</td>\n",
       "      <td>0.441435</td>\n",
       "      <td>0.519862</td>\n",
       "      <td>0.460810</td>\n",
       "      <td>0.492654</td>\n",
       "      <td>0.520750</td>\n",
       "      <td>0.507066</td>\n",
       "      <td>0.483851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578451</td>\n",
       "      <td>0.508495</td>\n",
       "      <td>0.516205</td>\n",
       "      <td>0.550025</td>\n",
       "      <td>0.575886</td>\n",
       "      <td>0.507974</td>\n",
       "      <td>0.571483</td>\n",
       "      <td>0.513574</td>\n",
       "      <td>0.522663</td>\n",
       "      <td>0.468337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472135</td>\n",
       "      <td>-0.064735</td>\n",
       "      <td>0.412660</td>\n",
       "      <td>-0.024795</td>\n",
       "      <td>0.211310</td>\n",
       "      <td>-0.054840</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>-0.048075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.332390</td>\n",
       "      <td>0.286435</td>\n",
       "      <td>-0.443165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.236885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.242595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.165350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.871110</td>\n",
       "      <td>0.016310</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.728730</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.684210</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553890</td>\n",
       "      <td>-0.015050</td>\n",
       "      <td>0.708240</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>0.496640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409560</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334655</td>\n",
       "      <td>0.969240</td>\n",
       "      <td>0.445675</td>\n",
       "      <td>0.953240</td>\n",
       "      <td>0.534195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905240</td>\n",
       "      <td>0.156765</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.153535</td>\n",
       "      <td>0.883465</td>\n",
       "      <td>0.154075</td>\n",
       "      <td>0.857620</td>\n",
       "      <td>0.200120</td>\n",
       "      <td>0.813765</td>\n",
       "      <td>0.171660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature1  feature2    feature3    feature4    feature5    feature6  \\\n",
       "count  351.000000     351.0  351.000000  351.000000  351.000000  351.000000   \n",
       "mean     0.891738       0.0    0.641342    0.044372    0.601068    0.115889   \n",
       "std      0.311155       0.0    0.497708    0.441435    0.519862    0.460810   \n",
       "min      0.000000       0.0   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%      1.000000       0.0    0.472135   -0.064735    0.412660   -0.024795   \n",
       "50%      1.000000       0.0    0.871110    0.016310    0.809200    0.022800   \n",
       "75%      1.000000       0.0    1.000000    0.194185    1.000000    0.334655   \n",
       "max      1.000000       0.0    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         feature7    feature8    feature9   feature10  ...   feature25  \\\n",
       "count  351.000000  351.000000  351.000000  351.000000  ...  351.000000   \n",
       "mean     0.550095    0.119360    0.511848    0.181345  ...    0.396135   \n",
       "std      0.492654    0.520750    0.507066    0.483851  ...    0.578451   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
       "25%      0.211310   -0.054840    0.087110   -0.048075  ...    0.000000   \n",
       "50%      0.728730    0.014710    0.684210    0.018290  ...    0.553890   \n",
       "75%      0.969240    0.445675    0.953240    0.534195  ...    0.905240   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "        feature26   feature27   feature28   feature29   feature30   feature31  \\\n",
       "count  351.000000  351.000000  351.000000  351.000000  351.000000  351.000000   \n",
       "mean    -0.071187    0.541641   -0.069538    0.378445   -0.027907    0.352514   \n",
       "std      0.508495    0.516205    0.550025    0.575886    0.507974    0.571483   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%     -0.332390    0.286435   -0.443165    0.000000   -0.236885    0.000000   \n",
       "50%     -0.015050    0.708240   -0.017690    0.496640    0.000000    0.442770   \n",
       "75%      0.156765    0.999945    0.153535    0.883465    0.154075    0.857620   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "        feature32   feature33   feature34  \n",
       "count  351.000000  351.000000  351.000000  \n",
       "mean    -0.003794    0.349364    0.014480  \n",
       "std      0.513574    0.522663    0.468337  \n",
       "min     -1.000000   -1.000000   -1.000000  \n",
       "25%     -0.242595    0.000000   -0.165350  \n",
       "50%      0.000000    0.409560    0.000000  \n",
       "75%      0.200120    0.813765    0.171660  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2e4A-64ee9M",
    "outputId": "5ee4747c-8110-4455-8149-cfcebb252fff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data.feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DQl8HTdLi7Hg"
   },
   "outputs": [],
   "source": [
    "data.sample(frac=0.4)\n",
    "data.drop(columns=[\"feature2\"] ,inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x7r6VCW0Qjh"
   },
   "source": [
    "# **Split into 60 and 40 ratio.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ez50Wv9UkIyi"
   },
   "outputs": [],
   "source": [
    "labels = data.pop(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0u1LWPpk2CdY"
   },
   "outputs": [],
   "source": [
    "data -= data.mean()\n",
    "data /= data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pWw7vo1wmsId"
   },
   "outputs": [],
   "source": [
    "labels = pd.Series([0 if lbl == 'g' else 1 for lbl in labels]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HncRgzlqk0Rd"
   },
   "outputs": [],
   "source": [
    "data_len = len(data)\n",
    "train_data = data.iloc[:data_len*60//100]\n",
    "test_data = data.iloc[data_len*60//100:]\n",
    "labels_len = len(labels)\n",
    "train_labels = labels.iloc[:labels_len*60//100]\n",
    "test_labels = labels.iloc[labels_len*60//100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqGCFS1h0Zru"
   },
   "source": [
    "# **Model : 1 hidden layers including 16 unit.  Train the Model with Epochs (100).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GC7FHwZnlB3_",
    "outputId": "e7e4fa58-5c18-4c21-c973-5f4e1d0e0f10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 10ms/step - loss: 1.0568 - acc: 0.4451\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.0511 - acc: 0.4035\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1009 - acc: 0.4197\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.9838 - acc: 0.4396\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0188 - acc: 0.4376\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9041 - acc: 0.4705\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8619 - acc: 0.5332\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8268 - acc: 0.5685\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9014 - acc: 0.5257\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8969 - acc: 0.5110\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7755 - acc: 0.5569\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7789 - acc: 0.5846\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8338 - acc: 0.5260\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7811 - acc: 0.5852\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8034 - acc: 0.5479\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7696 - acc: 0.5612\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7964 - acc: 0.5401\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7428 - acc: 0.5869\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7514 - acc: 0.5763\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7348 - acc: 0.6190\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7223 - acc: 0.6130\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6573 - acc: 0.6095\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6647 - acc: 0.6280\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6173 - acc: 0.6901\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7253 - acc: 0.6190\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6105 - acc: 0.7264\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6783 - acc: 0.6494\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6854 - acc: 0.6701\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6427 - acc: 0.6921\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6969 - acc: 0.6158\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6067 - acc: 0.7369\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6943 - acc: 0.6406\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6219 - acc: 0.6537\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6109 - acc: 0.6606\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6185 - acc: 0.6759\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6321 - acc: 0.6537\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5542 - acc: 0.7132\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5684 - acc: 0.7068\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5946 - acc: 0.7230\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5722 - acc: 0.7169\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5474 - acc: 0.7631\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5648 - acc: 0.7478\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5663 - acc: 0.7175\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5174 - acc: 0.7464\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4828 - acc: 0.7600\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5368 - acc: 0.7620\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5290 - acc: 0.7516\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5209 - acc: 0.7473\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4788 - acc: 0.7984\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4516 - acc: 0.8062\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5323 - acc: 0.7784\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5422 - acc: 0.6886\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4999 - acc: 0.765 - 0s 8ms/step - loss: 0.4883 - acc: 0.7695\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4880 - acc: 0.7684\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4736 - acc: 0.7952\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4825 - acc: 0.7727\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4789 - acc: 0.7863\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5217 - acc: 0.7484\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4646 - acc: 0.8021\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4294 - acc: 0.7906\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4780 - acc: 0.7730\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4822 - acc: 0.7553\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4205 - acc: 0.8163\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4644 - acc: 0.7891\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4713 - acc: 0.7663\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4151 - acc: 0.8062\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4573 - acc: 0.8235\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4265 - acc: 0.8131\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4797 - acc: 0.7842\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4472 - acc: 0.7874\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4365 - acc: 0.8316\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4621 - acc: 0.7995\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4577 - acc: 0.8088\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4449 - acc: 0.7764\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4190 - acc: 0.8189\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.8267\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4239 - acc: 0.8169\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3906 - acc: 0.8446\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4220 - acc: 0.8478\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4285 - acc: 0.8053\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4283 - acc: 0.8232\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4421 - acc: 0.8385\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4392 - acc: 0.7952\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4014 - acc: 0.8432\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4386 - acc: 0.8244\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3873 - acc: 0.8648\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3816 - acc: 0.8573\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3860 - acc: 0.8426\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3845 - acc: 0.8105\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3751 - acc: 0.8258\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4040 - acc: 0.8379\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3879 - acc: 0.8458\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3727 - acc: 0.8489\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3959 - acc: 0.8143\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3803 - acc: 0.8458\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3906 - acc: 0.8527\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4098 - acc: 0.8567\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3694 - acc: 0.8186\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3765 - acc: 0.8700\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3615 - acc: 0.8585\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(16,activation=\"relu\",input_shape=(len(train_data.columns),)))\n",
    "# network.add(layers.Dense(13,activation=\"relu\"))\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "network.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "network.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "with tf.device('/device:GPU:1'):\n",
    "  %time MODEL = network.fit(train_data,train_labels,epochs=100,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMPZ-IFe0muX"
   },
   "source": [
    "# **Evaluation Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80xJvwUapWBy",
    "outputId": "7e9c92a3-5afb-4f23-e732-330242ec79be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3784 - acc: 0.8794\n"
     ]
    }
   ],
   "source": [
    "loss , acc = network.evaluate(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmgzytJe0vhw"
   },
   "source": [
    "# **Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dTAo1hnp6QO",
    "outputId": "3a748089-167b-4336-85a1-22634f145d80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test_data = test_data[test_labels == 1]\n",
    "predictions = network.predict(prediction_test_data)\n",
    "len(np.round(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QVfBis200MY"
   },
   "source": [
    "# **Model : 2 hidden layers including 33 and 13 unit.  Train the Model with Epochs (80).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrrxAqmVslgZ",
    "outputId": "335e6ffd-2125-48a8-883d-d83e88980900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "4/4 [==============================] - 1s 3ms/step - loss: 0.8392 - acc: 0.5735\n",
      "Epoch 2/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6798 - acc: 0.6701\n",
      "Epoch 3/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6234 - acc: 0.6910\n",
      "Epoch 4/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5763 - acc: 0.7213\n",
      "Epoch 5/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5195 - acc: 0.7780\n",
      "Epoch 6/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4963 - acc: 0.7690\n",
      "Epoch 7/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4667 - acc: 0.7976\n",
      "Epoch 8/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4425 - acc: 0.8215\n",
      "Epoch 9/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3937 - acc: 0.8597\n",
      "Epoch 10/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4008 - acc: 0.8278\n",
      "Epoch 11/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3788 - acc: 0.8425\n",
      "Epoch 12/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3411 - acc: 0.8718\n",
      "Epoch 13/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3228 - acc: 0.8842\n",
      "Epoch 14/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3263 - acc: 0.8856\n",
      "Epoch 15/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3296 - acc: 0.8621\n",
      "Epoch 16/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3011 - acc: 0.8765\n",
      "Epoch 17/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2994 - acc: 0.8664\n",
      "Epoch 18/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2799 - acc: 0.8685\n",
      "Epoch 19/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2800 - acc: 0.8673\n",
      "Epoch 20/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2531 - acc: 0.8966\n",
      "Epoch 21/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2342 - acc: 0.9046\n",
      "Epoch 22/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2490 - acc: 0.8855\n",
      "Epoch 23/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2190 - acc: 0.9096\n",
      "Epoch 24/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2068 - acc: 0.9190\n",
      "Epoch 25/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2066 - acc: 0.9162\n",
      "Epoch 26/80\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2515 - acc: 0.859 - 0s 2ms/step - loss: 0.2185 - acc: 0.8980\n",
      "Epoch 27/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2098 - acc: 0.9086\n",
      "Epoch 28/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1940 - acc: 0.9258\n",
      "Epoch 29/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1850 - acc: 0.9268\n",
      "Epoch 30/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1780 - acc: 0.9346\n",
      "Epoch 31/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1847 - acc: 0.9211\n",
      "Epoch 32/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1764 - acc: 0.9242\n",
      "Epoch 33/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1680 - acc: 0.9258\n",
      "Epoch 34/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1566 - acc: 0.9305\n",
      "Epoch 35/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1703 - acc: 0.9162\n",
      "Epoch 36/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1529 - acc: 0.9421\n",
      "Epoch 37/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1576 - acc: 0.9270\n",
      "Epoch 38/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1465 - acc: 0.9379\n",
      "Epoch 39/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1361 - acc: 0.9390\n",
      "Epoch 40/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1475 - acc: 0.9332\n",
      "Epoch 41/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1505 - acc: 0.9289\n",
      "Epoch 42/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1178 - acc: 0.9513\n",
      "Epoch 43/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1465 - acc: 0.9289\n",
      "Epoch 44/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1241 - acc: 0.9419\n",
      "Epoch 45/80\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1576 - acc: 0.906 - 0s 3ms/step - loss: 0.1341 - acc: 0.9284\n",
      "Epoch 46/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1145 - acc: 0.9409\n",
      "Epoch 47/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1130 - acc: 0.9464\n",
      "Epoch 48/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1275 - acc: 0.9395\n",
      "Epoch 49/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1137 - acc: 0.9436\n",
      "Epoch 50/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1113 - acc: 0.9389\n",
      "Epoch 51/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1141 - acc: 0.9455\n",
      "Epoch 52/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1091 - acc: 0.9436\n",
      "Epoch 53/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1060 - acc: 0.9497\n",
      "Epoch 54/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0879 - acc: 0.9606\n",
      "Epoch 55/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0977 - acc: 0.9532\n",
      "Epoch 56/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1028 - acc: 0.9561\n",
      "Epoch 57/80\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0905 - acc: 0.9665\n",
      "Epoch 58/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0889 - acc: 0.9705\n",
      "Epoch 59/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0898 - acc: 0.9636\n",
      "Epoch 60/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0764 - acc: 0.9792\n",
      "Epoch 61/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0793 - acc: 0.9756\n",
      "Epoch 62/80\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0850 - acc: 0.9696\n",
      "Epoch 63/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0753 - acc: 0.9754\n",
      "Epoch 64/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0773 - acc: 0.9846\n",
      "Epoch 65/80\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0773 - acc: 0.9773\n",
      "Epoch 66/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0792 - acc: 0.9681\n",
      "Epoch 67/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0685 - acc: 0.9835\n",
      "Epoch 68/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0660 - acc: 0.9820\n",
      "Epoch 69/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0707 - acc: 0.9783\n",
      "Epoch 70/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0601 - acc: 0.9860\n",
      "Epoch 71/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0618 - acc: 0.9742\n",
      "Epoch 72/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0555 - acc: 0.9872\n",
      "Epoch 73/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0609 - acc: 0.9818\n",
      "Epoch 74/80\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0891 - acc: 0.968 - 0s 4ms/step - loss: 0.0641 - acc: 0.9818\n",
      "Epoch 75/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0571 - acc: 0.9833\n",
      "Epoch 76/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0469 - acc: 0.9880\n",
      "Epoch 77/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0495 - acc: 0.9875\n",
      "Epoch 78/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0443 - acc: 0.9922\n",
      "Epoch 79/80\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0556 - acc: 0.9833\n",
      "Epoch 80/80\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0495 - acc: 0.9896\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "network1 = models.Sequential()\n",
    "network1.add(layers.Dense(33,activation=\"relu\",input_shape=(len(train_data.columns),)))\n",
    "network1.add(layers.Dense(13,activation=\"relu\"))\n",
    "network1.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "network1.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "with tf.device('/device:GPU:1'):\n",
    "  %time MODEL1 = network1.fit(train_data,train_labels, validation_split=0.,epochs=80,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NZTu3pg1IO-"
   },
   "source": [
    "# **Training loss visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "5gmQ4jMqvbvO",
    "outputId": "3f4d2c84-f0b2-4419-f68b-008899319dd0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = MODEL1.history\n",
    "acc_values = history_dict['loss']\n",
    "epoches = np.arange(1,len(history_dict['acc'])+1)\n",
    "plt.plot(epoches,acc_values,'r',label=\"Training Accuracy\")\n",
    "plt.title('Training loss')\n",
    "plt.xlabel(\"Epoches\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5imVa-A1c9s"
   },
   "source": [
    "# **Evaluation Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n40sn567tUhB",
    "outputId": "a8d9bc34-9172-43b2-98a3-3bcff5e71bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0767 - acc: 0.9716\n"
     ]
    }
   ],
   "source": [
    "loss , acc = network1.evaluate(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJTWugDN1hTZ"
   },
   "source": [
    "# **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sJiWAhdWtC6o"
   },
   "outputs": [],
   "source": [
    "prediction_test_data = test_data\n",
    "predictions = network1.predict(prediction_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSwq41nKtIcV",
    "outputId": "ec4742f4-ef26-4e62-bd6c-d9ece37b49cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fanally Alhumdullah i got accuracy upto 97.16312289237976%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fanally Alhumdullah i got accuracy upto {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6u7OHEtwXHTz"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(predictions)\n",
    "test_labels2 = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOgNOnYC10DR",
    "outputId": "c8f12f4b-70eb-4ea9-e754-63b464cf1840"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[116,   3],\n",
       "       [  1,  21]])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(\n",
    "    test_labels2, predictions, num_classes=None, weights=None, dtype=tf.dtypes.int32,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "F8bNiPSu7MIm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ionosphere_Assignment5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
